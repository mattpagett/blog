{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9eb1772a",
   "metadata": {
    "time_run": "2026-02-03T18:23:09.351973+00:00"
   },
   "source": [
    "---\n",
    "title: \"Can We Use AI to Detect AI Bots on Social Media?\"\n",
    "date: \"2026-02-03\"\n",
    "categories: [AI, research]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f301c",
   "metadata": {
    "time_run": "2026-02-03T16:00:13.588501+00:00"
   },
   "source": [
    "**Short answer**: We can reliably detect obvious spam bots using large language models.  However, we do not know if we can reliably detect human-like AI accounts.\n",
    "\n",
    "This post summarizes our findings from the November 2025 Apart Research def/acc hackathon, during which Andreas Raaskov and I [tested](https://apartresearch.com/project/comparative-llm-methods-for-social-media-bot-detection-u9s4) whether large language models (LLMs) could help detect social media bots on Bluesky. One of our key findings: when bots are not obvious, the prompts we use to detect them strongly influence the detection results.\n",
    "\n",
    "Our central question: **Using only publicly visible content and metadata, can we detect an AI-generated account that mimics a human user?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebeefd",
   "metadata": {
    "time_run": "2026-02-04T00:18:23.452152+00:00"
   },
   "source": [
    "## In brief:\n",
    "- Existing algorithmic methods and our LLM-based methods worked well on **obvious spam and scam bots**.\n",
    "- These methods failed on the **hard cases**: accounts behaving like ordinary, engaged humans.\n",
    "- LLM-based detection varies wildly depending on prompt wording (from ~9% to ~91% of the same accounts flagged).\n",
    "- Without reliable **ground truth**, detection results are not interpretable.\n",
    "- A major risk today is **false positives**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a3282",
   "metadata": {
    "time_run": "2026-02-03T16:02:09.351069+00:00"
   },
   "source": [
    "## Why Bot Detection Is Getting Harder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649d2e8",
   "metadata": {
    "time_run": "2026-02-04T00:18:59.425064+00:00"
   },
   "source": [
    "Bot detection used to be straightforward. Automated accounts posted constantly, followed thousands of users at once, or repeated the same promotional text. Existing tools are very good at catching that behavior. We found that low-cost LLMs were also able to detect this kind of bot behavior.\n",
    "\n",
    "Recently, the quality of bot text generation has improved dramatically. Modern LLMs can produce short, context-appropriate replies that look indistinguishable from normal human participation, especially in settings like social media threads. An account powered by an LLM does not need to post frequently, coordinate with other accounts, or impersonate a specific individual to be disruptive. LLMs can simply participate.\n",
    "\n",
    "This change in bot text quality raises the question we set out to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da448244",
   "metadata": {
    "time_run": "2026-02-03T16:03:09.772460+00:00"
   },
   "source": [
    "## Our Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caf92f",
   "metadata": {
    "time_run": "2026-02-03T16:04:19.158680+00:00"
   },
   "source": [
    "To examine this question, we chose to look at Bluesky posts because its data is openly accessible via the AT Protocol and the platform functions as a high-engagement public discussion space.\n",
    "\n",
    "We evaluated four detection approaches:\n",
    "1. **Follower / following patterns**\n",
    "2. **Posting frequency and timing**\n",
    "3. **Text-based heuristics** (repetition, templating, typical AI phrases)\n",
    "4. **LLMs used as judges,** via zero-shot prompts (asking a model to judge without prior examples) asking whether an account appeared AI-generated\n",
    "\n",
    "A key challenge with LLM-based detection is **prompt sensitivity**: the exact wording of the question can dramatically change results. We tested multiple prompt variations to examine this effect.\n",
    "\n",
    "We tested these on two datasets:\n",
    "\n",
    "- **A known bot list,** consisting mostly of obvious spam and scam accounts\n",
    "- **A real-world sample** of 1,000 accounts that had replied to popular posts in active discussion threads\n",
    "\n",
    "The second sample was significantly more challenging to analyze, because it contained accounts that looked human-generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1ab1",
   "metadata": {
    "time_run": "2026-02-03T16:04:35.502653+00:00"
   },
   "source": [
    "## Obvious Cases Were Easily Detectable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca408",
   "metadata": {
    "time_run": "2026-02-03T16:04:51.759820+00:00"
   },
   "source": [
    "Against the known bot list, all of our methods worked.\n",
    "\n",
    "Algorithmic methods flagged accounts with non-human posting rates, suspicious follower ratios, and repetitive promotional content. LLM-based judges also detected nearly all of these accounts, across multiple prompt variations. Our findings aligned with prior work and deployed tools, including algorithmic bot detectors, keyword-based heuristics, and network analysis approaches, all of which are generally effective against high-volume spam and coordinated campaigns.\n",
    "\n",
    "This result is reassuring, but not surprising. These are exactly the kinds of bots that existing detection systems are designed to catch. LLMs turn out to be quite good at detecting them too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f8fc4",
   "metadata": {
    "time_run": "2026-02-03T16:05:07.887323+00:00"
   },
   "source": [
    "## Detection Failed for the Difficult Cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb1bf2",
   "metadata": {
    "time_run": "2026-02-03T16:06:17.519573+00:00"
   },
   "source": [
    "When we applied the same methods to the 1,000 accounts drawn from active reply threads, the results changed dramatically.\n",
    "\n",
    "### Algorithmic Methods:\n",
    "\n",
    "Algorithmic methods detected almost no bots in this sample.\n",
    "\n",
    "Follower ratios looked normal. Posting rates fell within human ranges. Text analysis found little repetition or templating. Either there were no sophisticated bots in this sample—or the bots were behaving exactly like humans.\n",
    "\n",
    "### LLM-Based Detection:\n",
    "\n",
    "LLM-based detection gave unstable responses.\n",
    "\n",
    "Different prompts produced radically different results:\n",
    "\n",
    "- One prompt flagged **~16%** of accounts as bots\n",
    "- Another flagged **~21%**\n",
    "- Another flagged **~29%**\n",
    "- A highly specific prompt flagged **~91%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f480e",
   "metadata": {
    "time_run": "2026-02-03T16:10:56.579358+00:00"
   },
   "source": [
    "![Bot detection infographic](bot_infographic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f3774",
   "metadata": {
    "time_run": "2026-02-03T16:13:36.484195+00:00"
   },
   "source": [
    "<center><i>The chart in the center shows how the same accounts receive dramatically different classifications depending only on prompt wording.</center></i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9b75a",
   "metadata": {
    "time_run": "2026-02-03T16:13:43.110263+00:00"
   },
   "source": [
    "## Prompt Sensitivity Is a Core Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46ec70",
   "metadata": {
    "time_run": "2026-02-03T16:14:16.311694+00:00"
   },
   "source": [
    "The variability in results reveals a deeper issue.\n",
    "\n",
    "When accounts fall into an ambiguous middle ground—posting frequently, expressing opinions, engaging in debate—LLMs do not have a stable basis for classification. Prompting becomes *prompt steering*: asking the model to be more or less suspicious directly changes the outcome.\n",
    "\n",
    "In one case, an account was flagged as “likely AI” by all four prompts. Manual review suggested the posts came from a politically engaged human who posting frequently and contextually. The LLM had mistaken this user's enthusiasm for automation.\n",
    "Without ground truth, we cannot tell which prompt, if any, is accurate.\n",
    "\n",
    "- Is 91% detection uncovering hidden bots?\n",
    "- Or is it massively over-flagging humans?\n",
    "- Is 16% conservative and accurate—or simply missing things?\n",
    "\n",
    "There is no way to know which method is accurate without ground truth - and how can we assess a ground truth about bots designed to be undetectable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06fbf0",
   "metadata": {
    "time_run": "2026-02-03T16:14:32.863623+00:00"
   },
   "source": [
    "## Why This Matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310976e8",
   "metadata": {
    "time_run": "2026-02-03T16:14:50.202526+00:00"
   },
   "source": [
    "Two risks: one is that undetected bots flood social media spaces, influencing conversations in ways completely unknown to humans on the platform - a major concern.  Another risk is **false positives**. If detection systems cannot distinguish between human-like AI behavior and normal human participation, they will inevitably mislabel real users—especially those who post frequently, argue passionately, or communicate in unusual styles.  Both of these risks pose dangers for platform governance, and user trust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea49b8",
   "metadata": {
    "time_run": "2026-02-03T16:15:01.653570+00:00"
   },
   "source": [
    "## The Real Bottleneck: Ground Truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a925ba8",
   "metadata": {
    "time_run": "2026-02-03T16:15:33.825879+00:00"
   },
   "source": [
    "The fundamental barrier for our research is the absence of reliable examples of bots that act like humans.\n",
    "\n",
    "We do not have a validated dataset of human-like, LLM-generated social media accounts that:\n",
    "\n",
    "- Behave organically over time\n",
    "- Show normal engagement patterns\n",
    "- Avoid obvious spam or coordination signals\n",
    "\n",
    "Without such a reference, detection results cannot be calibrated, compared, or meaningfully evaluated.\n",
    "\n",
    "**In short**: Our intervention was to test LLM-based detection against realistic, ambiguous accounts rather than obvious bots. The main problem we face now is a lack of reliable reference points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ac91f",
   "metadata": {
    "time_run": "2026-02-03T16:15:50.602895+00:00"
   },
   "source": [
    "## What We're Doing Next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e15fe",
   "metadata": {
    "time_run": "2026-02-03T16:16:50.057645+00:00"
   },
   "source": [
    "Our next step is to build a **synthetic reference dataset**—not to deploy bots on live platforms, but to generate realistic posting histories offline.\n",
    "\n",
    "The goal is to create controlled examples of accounts that are:\n",
    "\n",
    "- Indistinguishable from humans to casual inspection\n",
    "- Generated using multiple models and prompting styles\n",
    "- Accompanied by realistic metadata and engagement patterns\n",
    "\n",
    "With such a dataset, we can ask a meaningful question: **are there any signals—textual or behavioral—that consistently distinguish human expression from AI-generated participation?**\n",
    "\n",
    "It is possible the answer will be “no.” In that case, knowing that we cannot detect LLM accounts will also be an important finding.\n",
    "\n",
    "Future extensions to this research that we have considered but have not yet attempted include:\n",
    "\n",
    "- Larger samples across different languages and communities\n",
    "- Few-shot or fine-tuned detection models (using additional training methods to improve the LLM’s detection behavior)\n",
    "- Broader network-level analysis combined with content analysis - potentially partnering with platform operators\n",
    "\n",
    "We intentionally scoped this work to evaluate *zero-shot LLM judgment*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706b95e",
   "metadata": {
    "time_run": "2026-02-03T19:20:59.196550+00:00"
   },
   "source": [
    "## How This Fits with Prior Work\n",
    "[Prior work](https://dl.acm.org/doi/10.1007/s00521-023-08352-z) on social media bot detection has focused largely on algorithmic signals (such as posting frequency or network structure) and, [more recently](https://arxiv.org/abs/2402.00371), on deep learning classifiers trained on labeled datasets. These approaches work well for coordinated campaigns and spam bots, but they assume access to reliable ground truth.\n",
    "\n",
    "Recent experimental work has also shown that modern LLMs can pass controlled [Turing-style evaluations](https://arxiv.org/abs/2503.23674), suggesting that text alone may no longer reliably distinguish humans from AI.  Work has also been done to show that LLMs can be effective at [zero-shot anomaly detection](https://arxiv.org/abs/2402.10350), which we think will be useful to pursue in this context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58957dd",
   "metadata": {
    "time_run": "2026-02-03T18:03:15.008197+00:00"
   },
   "source": [
    "## Limits and Open Questions\n",
    "Our work does not show that sophisticated LLM bots are currently operating on Bluesky. If they are, we have not yet found a reliable method for detecting them. Our work shows how fragile current approaches are when applied to real-world, ambiguous cases.\n",
    "\n",
    "There is also a deeper uncertainty: it may be that human and AI-generated social media behavior has already converged to the point where reliable distinction is impossible using content alone, raising the possibility that human and AI social media behavior may already be converging in these contexts.\n",
    "\n",
    "Because we lack verified examples of human-like AI accounts, we cannot estimate detection accuracy yet.  Testing that is the point of the next phase of research.\n",
    "\n",
    "**Ethical note**: We deliberately avoided publishing lists of flagged accounts, with the exception of referencing a published known botlist. Our results showed high false-positive risk, including misclassifying human users as bots. Any future dataset or detection work must prioritize harm minimization, anonymization, and informed consent, especially when classifications can affect real people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61db4f3",
   "metadata": {
    "time_run": "2026-02-03T18:03:57.271103+00:00"
   },
   "source": [
    "## Bottom Line\n",
    "\n",
    "Current detection methods are effective against obvious bots and spam. These detection methods struggle where the problem matters most - ambiguous cases. This problem will be exacerbated as LLM quality improves.\n",
    "\n",
    "Open questions:\n",
    "- Are there any stable signals that distinguish human and AI participation at all? If so, might these vanish in the future?\n",
    "- Will platforms need to shift from detection to disclosure or provenance mechanisms?\n",
    "- How could moderation systems handle inevitable ambiguity without harming users?\n",
    "\n",
    "These are gaps our research aims to help close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875783f",
   "metadata": {
    "time_run": "2026-02-03T19:23:18.216183+00:00"
   },
   "source": [
    "## Code: \n",
    "\n",
    "[Github repo for the Hackathon project](https://github.com/AndreasRaaskov/Bot-Detector)\n",
    "\n",
    "Contact: matt@mattpagett.dev\n",
    "\n",
    "We'd love to hear your ideas. If you're working on related problems or have thoughts on approaches we should try, please reach out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f61cf0",
   "metadata": {
    "time_run": "2026-02-03T18:16:50.023456+00:00"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We would like to thank Apart Research, BlueDot Impact, and Halcyon Futures for hosting the hackathon that initiated this project. Lambda.ai provided compute credits. Li-Lian Ang, Mackenzie Puig-Hall, and the Apart Lab Studio provided valuable feedback on our initial draft.\n",
    "\n",
    "Tools used in writing this post: Claude Opus 4.5, Claude Sonnet 4.5, ChatGPT 5.2, NotebookLM, and SolveIt.\n"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
